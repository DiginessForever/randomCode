<!DOCTYPE html>
<meta charset="utf-8" />
<html>
        <head>
                <script>
 //This script contains the javascript "classes" (functions that return an object instance) and utility functions for a generic
   //feedforward/backpropagation artificial neuron net (ANN).  If you make many layers then it's a deep learning network.
    //This is my first implementation, so it will be basic and it will not necessarily have the most efficient means of doing it.
   //Note - I've been reading about neural nets for quite some time now, to include history on Wikipedia and watching videos by professors from MIT,
   // from India, and on Andrew Ng's Coursera machine learning course, in addition to a video tutorial by Pete Warden and many others.
   //  (hundreds of hours total, spent trying to grok neural nets).
   //Of particular note are:
   // Andrew Ng, Geoffrey Hinton, (Yann Lecun is notable but goes right over my head), Pete Warden and Dave Miller for their video tutorials.
   // It all didn't quite click for me enough to actually implement until I watched this
   // Youtube video series:  Neural Networks Demystified by "Welch Labs":  https://www.youtube.com/watch?v=bxe2T-V8XRs
   //I got stuck again when I started trying to implement backpropagation.  The Welch Labs video lost me there.  I got back on track again with
   // this video by Geoffrey Hinton:  https://www.youtube.com/watch?v=46Jzu-xWIBk
   // Watch the Welch Labs video on backprop first, don't worry if you get lost, then follow it up with Geoffrey Hinton's video above.
   //  Note:  I think my neuron values are the same thing as "Activations" that Hinton mentions in his video.

   //The professor who finally got me over the last hump on backpropagation with his very clearly written code on his university page is:
   //John A. Bullinaria from the School of Computer Science of The University of Birmingham, UK:
   //http://www.cs.bham.ac.uk/~jxb/NN/nn.html

   //Special thanks to Pete Warden for implementing the deep belief SDK for the Raspberry Pi enabling use of its GPU,
   //https://github.com/jetpacapp/DeepBeliefSDK/tree/master  (and he made an assembler for that.
   //This one was made by someone else who forked his assembler:  https://github.com/p1er/vc4asm)
   //Thanks also to Elon Musk for providing extra motivation by creating OpenAI, which released OpenAI Gym only 5 months after it was founded.
   //OpenAI Gym provides an environment to test out your neural nets against various tasks.  It has a special environment for each task so that
   //you can see exactly what's going on:  

   //And God said "Give me a whole lotta e!  Because you gotta e!"
   //What, you gotta eat?  "No, you gotta e!"
   //For more information about calculating e (an explanation of the process, explaining the n and e variables and the equation), see:
   //http://webcache.googleusercontent.com/search?q=cache:DEDdypvJtpUJ:www.mathsisfun.com/numbers/e-eulers-number.html+&cd=2&hl=en&ct=clnk&gl=us
   //The above is the Google cached page, and the below is the url to the site itself:
   //www.mathsisfun.com/numbers/e-eulers-number.html
   //var n = 1000;
   //var e = (1 + 1/n)^n;  //This is Euler's famous e, and the equation to calculate it to an aribitrary precision n.
   //Alternatively, I can avoid calculating e altogether (because it is Math.exp(1)), and just use the Math.exp() method for everything:
   //Here's the new activation function:  activationValue = 1/(1 - Math.exp(-z));  (where z is the sum from the weights * neuron values).
   //Note that Math.exp should be faster than doing the above...

   //See this video at time 1:27 for this derivative:  https://www.youtube.com/watch?v=tf9p1xQbWNM
   var derivativeOfActivationFunction = activationFunctionValue * (1 - activationFunctionValue);

   //Going to try to use this with array literals to get an arbitrary length array
   //without using "new Array(x)".
   function getRandomValuesAsString(numberOfValues){
       var returnString = "";
       while (numberOfValues > 0){
           returnString += Math.random(0,1);
           if (numberOfValues > 1){
              returnString += ",";
           }
           numberOfValues -= 1;
       }
   }

   function getZeros(numberOfValues){
	var returnString = "";
        while (numberOfValues > 0){
            returnString += "0";
            if (numberOfValues > 1){
            	returnString += ",";
            }
            numberOfValues -= 1;
        }
   }

   function neuronFactory(numberOfWeightsPerNeuron){
	this.neuron = {
		this.activationValue : Math.random(0,1);
		this.weights : [getRandomValuesAsString(numberOfWeightsPerNeuron)],
		this.weightDeltas : [getZeros(numberOfWeightsPerNeuron)]
	}
	return this.neuron;
   }

   function layerFactory(numberOfNeurons, numberOfWeightsPerNeuron){
        this.layer = {
                this.neurons : [numberOfNeurons],
		this.layerDelta : '0'
        }

	for (i = 0; i < numberOfNeurons; i++){
		this.layer.neurons[i] = neuronFactory(numberOfWeightsPerNeuron);
		this.layer.neurons.push(neuron);
	}

        return this.layer;
   }

   //Note:  In most of my code here, I am writing for a single training example.  I will have to change this to train against many training
   //             samples, so the "expectedOutput" variable below will go away (and something else will replace it, like the folder location of all the
   //             sample images.  Right now "expectedOutput" is supposed to be the training sample image.
   function networkFactory(numberOfHiddenLayers, numberOfNeuronsInHiddenLayers, numberOfNeuronsInOutputLayer, numberOfInputNeurons){
        var this.network = {
                this.layers : [2 + numberOfHiddenLayers],
                this.error : 0  //This is the current error, which is the MSE between the actual and expected output.
                                //See the calculateError() function for further details, it is heavily commented.
                                //Note: this is used to decide when to stop feedforward / backprop cycles (these cycles are called epochs).
                                //In order to stop, this error needs to be low enough (it'll never go all the way to zero).
        }
	
	//Set up the input layer.  Here we give it random values.  We will later need to load training samples to the input layer in order to
	//train the neural network.
        this.network.layers[0] = layerFactory(numberOfInputNeurons, numberOfNeuronsInHiddenLayers);

        for (i = 1; i <= numberOfHiddenLayers; i++){
                this.network.layers[i] = layerFactory(numberOfNeuronsInHiddenLayers, numberOfNeuronsInHiddenLayers);
        }

        var outputLayer = LayerFactory(numberOfNeuronsInOutputLayer, 0);

	//Set up the output layer (with random values).
        this.network.layers[this.network.layers.length - 1] = layerFactory(numberOfNeuronsInOutputLayer, 1); //I need just one weight per, in order
													     //to hold the delta value.
        return this.network;
   } //end networkFactory()


   //This function is the main control loop for feed forward (used in both training and operation of the neural net).
   //It calls the feed foward helper function below for each layer in the network
   //in the proper order:  hidden1, hidden2, hidden3, ..., output.  (never on the input layer because those neurons don't get changed by
   //anything but new incoming values from outside the network)
   function feedForward(network){
        //Because the first network layer (input) is at the i = 0 position, I'm skipping it, as we don't run the feedforward on the input layer.
        //This is because the way I've written the feedforward, it's gathering values into a layer from the previous layer.  The input layer
        //has no previous layer.  So we start at i = 1.
        for (i = 1; i < network.layers.length; i++){
            runLayerFeedForward(network.layers[i], network.layers[i - 1]);
        }
   }


   //This function feeds forward input values by iterating through each neuron in a layer.
   //It's actually getting values from the layer before the one it's working on.  This function should therefore never be run on the input layer.
   function runLayerFeedForward(thisLayer, previousLayer){
        for (i = 0; i < thisLayer.neurons.length; i++){
            thisLayer.neurons[i].activationValue = runNeuronActivationFunction(i, previousLayer);
        }
   }


   //This function will be ran once for each neuron in a layer when feeding forward.  The new value of the neuron we are running this function for
   // will be the return value of this function.
   //TODO:  Replace the multiplications in the for loop with just placing the elements in two arrays then using a more efficient matrix multiplication
   // See the following javascript numpy replacement:  https://github.com/nicolaspanel/numjs
   function runNeuronActivationFunction(thisNeuronLayerPosition, previousLayer){
        var z = 0;
        for (i = 0; i < previousLayer.neurons.length; i++){
                z += previousLayer.neurons[i].activationValue * previousLayer.neurons[i].weights[thisNeuronLayerPosition];
        }
        //This is a basic activation function for a neuron.  We mapped the current neuron to all the neurons connected to it from the previous layer
        //in the for loop above.
        //return 1/(1 + (1 / e ^ z));
     	return (1/(1 + Math.exp(-z))); //using Math.exp is faster than using powers, at least according to a stack overflow thread I found.
   }


   //This is how you "train" an artificial neural network, by backpropagating error after calculating an error value
   // (the difference between expected and actual values at the output layer after doing feedforward).
   function calculateError(network, expectedValues){
        //See this video by Jeff Heaton on how to calculate error:
        //https://www.youtube.com/watch?v=U4BTzF3Wzt0
        //Three methods:  Mean Squared Error (MSE), Root Mean Squared Error (RMSE or RMS), and ArcTan Error (used with quickprop).
        //Mean Squared Error:  (ideal value - actual value1)^2 + (ideal value2 - actual value2)^2  .... (ideal value n - actual value n)^2,
        //                      all divided by n (the number of samples)
        //Root Mean Squared Error:  take the square root of the above (the MSE).
        //ArcTan Error:  arctan^2(ideal value1 - actual value1) + arctan^2(ideal value2 - actual value2) + arctan^2(ideal value n - actual value n)
        //                      all divided by n.
        //Finally, most Youtube tutorials use a slightly modified MSE for the error function (they take half the MSE):  error = (1/2) * MSE.
        //This video is a good example:  https://www.youtube.com/watch?v=gl3lfL-g5mA

        //So I'm using the last example, the slightly modified MSE function, which is the MSE * 1/2 (or MSE / 2).

	outputNeurons = network.layers[network.layers.length - 1].neurons;   	//point at the output layer in order to use it for the next loop 
								    		//(it's the last layer on the network)
	var actualOutput = 0;
	var differenceActualVersusExpected = 0;
        for (i = 0; i < outputNeurons.length; i++){
		actualOutput = outputNeurons[i].activationValue;
		idealOutput = expectedValues[i];  //TODO:  Either make sure the lengths match or figure out a mapping.
						  //TODO:  Also, still need a training sample image loader (these will be the expectedValues).

		differenceActualVersusExpected = idealOutput - actualOutput;
                network.error += 0.5 * (differenceActualVersusExpected)^2 //TODO:  check this again!

		outputNeurons[i].deltas = differenceOfActualOutputNeuronFromExpected * actualOutput[i] * (1 - actualOutput[i]);
			//Note:  the output layer is the only one with a single primitive value in the neuron.deltas variable - for the other layers,
			//that is an array with one delta value for every weight.  That represents the change for that particular weight in the
			//direction which would minimize the error for the network.  (and that is the essence of gradient descent)
        }

        network.error = 1/2 * (network.error / actualOutput.length);  //This is the "all divided by n" step in the MSE equation above.
        //This is the MSE * 1/2 step from the "slightly modified MSE" equation above.
	//Note:  TODO: Make sure that we really need to multiple by 1/2 or .5 twice...
   }


   //So, these are the Youtube vids that I have used so far when trying to understand this algorithm (this time, there's been previous attempts):
   //https://www.youtube.com/watch?v=SvAEX5taVKk&ebc=ANyPxKpYJwxAESmptGGPmHGel1bqiHG45KiqICmAbcjzVTv8XyxP-CfNtcYY4A3NglKsgb7xWuSn
   //https://www.youtube.com/watch?v=IruMm7mPDdM&ebc=ANyPxKousVVr3_y4WYTYUDF693WR6aJPfpZHUrTYph5LzDNAYftwXqtXPG8-dR9AocEiXxfjiMjD
   //https://www.youtube.com/watch?v=gl3lfL-g5mA  and his code repository on github: https://github.com/SullyChen/FFANN/blob/master/cpps/FFANN.cpp
   //https://www.youtube.com/watch?v=GlcnxUlrtek  (really too fast for me, not enough detail due to currently weak calculus ability)
   //https://www.youtube.com/watch?v=UnWL2w7Fuo8  IIT lecture.
        //I really like the way that professor lays out the math and the graphs.  In spite of my weak calculus, I understand perfectly.
        //This is "Prof. P. Dasgupta" (that's what's in the Youtube video description, don't know his first name).
        //He's using 1/2 Root Means Squared for his activation function.  I'm keeping 1/2 MSE for mine.  Any of the methods will work.
   function backpropagate(network){
        var outputLayerIndex = network.layers[network.layers.length - 1];  //Point at the output layer to use as an index for the loop below.

	//Work backwards from the last hidden layer and output layer.
	for (i = outputLayerIndex - 1; i >= 0; i--){
		backpropagateLayer(network.layers[i], network.layers[i + 1]);
	}
   }

   //Now since we are back propagating the error, the previous layer is actually the layer to the right.
   //So, if we are at the last hidden layer, then the previous layer would be the output layer.
   //We continue like this, getting new deltas for each layer's neuron, all the way to the input layer, and update the input layer's deltas too.
   function backpropagateLayer(thisLayer, previousLayer){
	var sum = 0;
        for (i = 0; i < thisLayer.neurons.length; i++){
                sum = backpropagateNeuron(i, previousLayer);
		for (j = 0; j < previousLayer.neurons.length; j++){
			thisLayer.neurons[i].delta[j] += sum * previousLayer.neurons[j].activationValue * 
								(1 - previousLayer.neurons[j].activationValue);
		}
        }
	adjustLayerWeights(thisLayer, previousLayer);
   }

   /* Backprop the delta[k, j, whatevers] through the hidden layers:
	foreach thisLayerNeuron {
		foreach previousLayerNeuron{
			sum += previousLayer.neurons[i].delta * thisLayer.neurons[thisLayerNeuronPosition].weights[i]  //TODO double check indexing
		}
		foreach previousLayerNeuron{
			previousLayer.delta[h] = sum * previousLayerNeuron * (1 - previousLayerNeuron);
		}
	}
	Update the weights after backprop:
		foreach layer (inputLayer + 1 on){
			foreach neuron in layer {	
				weightOfPreviousLayerNeuronToThisNeuron += stepSize * thisLayer.neurons[thisLayerNeuron] + 
					(alphaLearningRate * thisLayer.neurons[thisLayerNeuron];  //TODO this seems somewhat incomplete - check again
			}
		}
   */

   function backpropagateNeuron(thisLayerNeuronPosition, thisLayerNeuron, previousLayer){
        var sum = 0;
        for (i = 0; i < previousLayer.length; i++){
                sum += previousLayer.neurons[i].deltas[thisLayerNeuronPosition] * thisLayerNeuron.weights[i];   
			//TODO:  Double check indexing.   //TODO:  Make sure the different handling for output layer is handled separately.
        }
        //newNeuronValue = newNeuronValue / previousLayer.length;   //(I don't see anything about this step being applied, removing)...
	//TODO:  double check that the above step is not present.
        return sum;
   }

   function adjustLayerWeights(thisLayer, previousLayer){
	for (i = 0; i < thisLayer.neurons.length; i++){
		adjustNeuronWeights(thisLayer.neurons[i], previousLayer);
	}
   }

   function adjustNeuronWeights(thisLayerNeuron, previousLayer){
	var stepSize = 0.7  	//TODO:  Need to read a ton more about this, the Lipschitz Continuity for exponential functions, etc...
				//Another name for stepSize seems to be "variance".
				//TODO:  Add logic to decrease stepsize.  Question:  would the stepsize ever reverse direction if it overshoots?
	var alpha = 0.7  	//TODO:  This is the learning rate.  Figure out how it works with step size (or variance?), and typical values.
				//	 It seems strange that they are both 0.7...I think the learning rate may need to be variable as well,
				//	 slowing down as the net settles on a solution.  More reading required.
	for (i = 0; i < previousLayer.neurons.length; i++){
		thisLayerNeuron.weights[i] += stepSize * previousLayer.neurons[i].activationValue + alpha * thisLayerNeuron.delta
	}

   }


   function trainNetwork(){
        //Going to use the lowest resolution image possible to try to avoid out of memory errors and speed up computation with the CPU.
        //Later, will try to use the GPU as well, then test ways to make the layers bigger so I can use higher resolution images.
        var resolutionOfImage = (320 * 240);
        var image = <path to image>;  //change to valid file path or URL
        prepareImagePixelRangesForInputLayer(image);
        var expectedImage = <path to image>; //change to a valid file path or URL
        prepareImagePixelRangesForInputLayer(expectedImage);

        //Just copying the network constructor signature here for reference:  networkFactory(numberOfHiddenLayers, numberOfNeuronsInInputLayer, numberOfNeuronsInHiddenLayers, numberOfNeuronsInOutputLayer, input, expectedOutput)
        var ANN = networkFactory(5, resolutionOfImage, resolutionOfImage /* this is entirely questionable memory-wise */, 1, image, expectedImage)
        //Some possible configurations of the output layer:  I could have it give me confidence of one letter with a single output neuron.
     //Alternatively, I could have a neuron for each letter, or each letter and each number.  For the Chinese Mandarin language, this would imply
        //hundreds of thousands of output neurons, so not practical.  I will probably have to wait until I learn about stacked/hierarchical neural nets
        //to do that.

        //For each training sample I give the neural network, I need to feedforward, calculate error, and backpropagate many times until
        //the I know the network has converged or failed to converge (found a minima that it wants to stay at).  If it fails to converge (diverges,
        //which means it's bouncing around), then I can start reducing the training rate or fiddle with the momentum.  Failing that, I can
        //start reinitializing the neural net with new random values and try again a bunch of times.  Maybe I could make an engine that
        //continuously tries those steps until it finds a good minima (which is something I'll have to make a test for).
        //Note:  continuously trying again with new random values and then testing is essentially a genetic algorithm.
        //The tests for genetic algorithms have to be written separately and are not generalized tests or a standard equation.
        //Make the network feedforward:
        runNetworkFeedForward(ANN);

        //Calculate error here.


        //Backpropagate the error here:


   } //end trainNetwork()


   //It's always good to have some convenience functions.
   //This function outputs the layers (neuron activation values) and their weights as a JSON formatted object
   function outputNetworkJSON(){
       

   }


   function prepareImagePixelRangesForInputLayer(image){
        //This function changes the grayscaled pixel values from a range of 0-255 to a range of 0-1 while keeping their relative values intact.
        for (i = 0; i < image.length; i++){
                image[i] = image[i] / 255;
        }
   } //end prepareImagePixelRangesForInputLayer()

                </script>
        </head>
        <body>
                <canvas id="Image"></canvas>
                <canvas id="InputLayer"></canvas>
                <canvas id="HiddenLayer1"></canvas>
                <canvas id="HiddenLayer2"></canvas>
                <canvas id="HiddenLayer3"></canvas>
                <canvas id="OutputLayer"></canvas>
        </body>
</html>
